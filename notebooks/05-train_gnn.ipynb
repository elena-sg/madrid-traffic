{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bened\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from graph_traffic.dataloading import graph_dataset, npzDataset\n",
    "from graph_traffic.dcrnn import DiffConv\n",
    "from graph_traffic.config import project_path\n",
    "from graph_traffic.model import GraphRNN\n",
    "from graph_traffic.train import train, eval\n",
    "from graph_traffic.utils import NormalizationLayer, masked_mae_loss\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import torch\n",
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Define training parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n_points = 10000\n",
    "dataset_name = \"madrid\"\n",
    "batch_size = 64\n",
    "diffsteps = 2\n",
    "decay_steps = 2000\n",
    "lr = 0.01\n",
    "minimum_lr = 2e-6\n",
    "epochs = 100\n",
    "max_grad_norm = 5.0\n",
    "num_workers = 0\n",
    "model = \"dcrnn\"\n",
    "gpu = -1\n",
    "num_heads = 2 # relevant for model=\"gaan\"\n",
    "out_feats = 256\n",
    "num_layers = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if gpu == -1:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cuda:{}'.format(gpu))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "g = graph_dataset(dataset_name)\n",
    "train_data = npzDataset(dataset_name, \"train\", n_points)\n",
    "test_data = npzDataset(dataset_name, \"test\", n_points)\n",
    "valid_data = npzDataset(dataset_name, \"valid\", n_points)\n",
    "\n",
    "seq_len = train_data.x.shape[1]\n",
    "in_feats = train_data.x.shape[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(4993, 12, 5, 2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "normalizer = NormalizationLayer(train_data.mean, train_data.std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "if model == \"dcrnn\":\n",
    "    batch_g = dgl.batch([g] * batch_size).to(device)\n",
    "    out_gs, in_gs = DiffConv.attach_graph(batch_g, diffsteps)\n",
    "    net = partial(DiffConv, k=diffsteps, in_graph_list=in_gs, out_graph_list=out_gs)\n",
    "elif model == 'gaan':\n",
    "    print(\"not available\")\n",
    "\n",
    "dcrnn = GraphRNN(in_feats=in_feats,\n",
    "                 out_feats=out_feats,\n",
    "                 seq_len=seq_len,\n",
    "                 num_layers=num_layers,\n",
    "                 net=net,\n",
    "                 decay_steps=decay_steps).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Define learning parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dcrnn.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "loss_fn = masked_mae_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DespuÃ©s de hacer el cambio sigmoid -> tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 266.5638197057833 Valid Loss: 262.79227326094036 Test Loss: 268.1165119622273\n",
      "Epoch: 9 Train Loss: 266.5960756501041 Valid Loss: 262.6397631446307 Test Loss: 268.3928483442049\n",
      "Epoch: 10 Train Loss: 265.90474781037557 Valid Loss: 262.5918209236464 Test Loss: 269.1274005343653\n",
      "Epoch: 11 Train Loss: 267.8067512259211 Valid Loss: 262.5564204427837 Test Loss: 268.3231428251254\n",
      "Epoch: 12 Train Loss: 266.7422842779866 Valid Loss: 262.5454366798092 Test Loss: 269.5108677077794\n",
      "Epoch: 13 Train Loss: 266.65345807973296 Valid Loss: 262.5086409722154 Test Loss: 267.63029021662504\n",
      "Batch:  78\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      2\u001B[0m     train(dcrnn, g, train_loader, optimizer, scheduler, normalizer, loss_fn, device, batch_size, max_grad_norm, minimum_lr)\n\u001B[1;32m----> 3\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43meval\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdcrnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(dcrnn, g, valid_loader, normalizer, loss_fn, device, batch_size)\n\u001B[0;32m      5\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(dcrnn, g, test_loader, normalizer, loss_fn, device, batch_size)\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\train.py:76\u001B[0m, in \u001B[0;36meval\u001B[1;34m(model, graph, dataloader, normalizer, loss_fn, device, batch_size)\u001B[0m\n\u001B[0;32m     74\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m batch_size\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (x, y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m---> 76\u001B[0m     y, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y)\n\u001B[0;32m     78\u001B[0m     total_loss\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(loss))\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\train.py:44\u001B[0m, in \u001B[0;36mpredict\u001B[1;34m(x, y, batch_size, graph, model, device, normalizer)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(x, y, batch_size, graph, model, device, normalizer):\n\u001B[0;32m     43\u001B[0m     x, y, x_norm, y_norm, batch_graph \u001B[38;5;241m=\u001B[39m prepare_data(x, y, batch_size, graph, normalizer, device)\n\u001B[1;32m---> 44\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_cnt\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     output \u001B[38;5;241m=\u001B[39m output[:, :, [\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;66;03m# Denormalization for loss compute\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\model.py:219\u001B[0m, in \u001B[0;36mGraphRNN.forward\u001B[1;34m(self, g, inputs, teacher_states, batch_cnt, device)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, g, inputs, teacher_states, batch_cnt, device):\n\u001B[1;32m--> 219\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(g, teacher_states, hidden, batch_cnt, device)\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\model.py:201\u001B[0m, in \u001B[0;36mGraphRNN.encode\u001B[1;34m(self, g, inputs, device)\u001B[0m\n\u001B[0;32m    198\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mzeros(g\u001B[38;5;241m.\u001B[39mnum_nodes(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_feats)\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m    199\u001B[0m     device) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers)]\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_len):\n\u001B[1;32m--> 201\u001B[0m     _, hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\model.py:90\u001B[0m, in \u001B[0;36mStackedEncoder.forward\u001B[1;34m(self, g, x, hidden_states)\u001B[0m\n\u001B[0;32m     88\u001B[0m hiddens \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers):\n\u001B[1;32m---> 90\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     hiddens\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x, hiddens\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\model.py:41\u001B[0m, in \u001B[0;36mGraphGRUCell.forward\u001B[1;34m(self, g, x, h)\u001B[0m\n\u001B[0;32m     38\u001B[0m r \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mr_net(\n\u001B[0;32m     39\u001B[0m     g, torch\u001B[38;5;241m.\u001B[39mcat([x, h], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mr_bias)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# update gate: how much of the past information needs to be passed along to the future\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m u \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mu_net\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mu_bias)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# current memory content: store relevant information from the past\u001B[39;00m\n\u001B[0;32m     44\u001B[0m h_ \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m*\u001B[39mh\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\dcrnn.py:107\u001B[0m, in \u001B[0;36mDiffConv.forward\u001B[1;34m(self, g, x)\u001B[0m\n\u001B[0;32m    105\u001B[0m         \u001B[38;5;66;03m# Each feat has shape [N,q_feats]\u001B[39;00m\n\u001B[0;32m    106\u001B[0m feat_list\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject_fcs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m](x))\n\u001B[1;32m--> 107\u001B[0m feat_list \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeat_list\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mview(\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;28mlen\u001B[39m(feat_list), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_feats)\n\u001B[0;32m    109\u001B[0m ret \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerger\u001B[38;5;241m*\u001B[39mfeat_list\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m))\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    train(dcrnn, g, train_loader, optimizer, scheduler, normalizer, loss_fn, device, batch_size, max_grad_norm, minimum_lr)\n",
    "    train_loss = eval(dcrnn, g, train_loader, normalizer, loss_fn, device, batch_size)\n",
    "    valid_loss = eval(dcrnn, g, valid_loader, normalizer, loss_fn, device, batch_size)\n",
    "    test_loss = eval(dcrnn, g, test_loader, normalizer, loss_fn, device, batch_size)\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Valid Loss: {valid_loss} Test Loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de hacer el cambio sigmoid -> tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bened\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 0 Train Loss: 417.7223258760343 Valid Loss: 411.6834200720523 Test Loss: 434.8402419671191\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 1 Train Loss: 325.0046306046629 Valid Loss: 329.22103194808915 Test Loss: 314.11694634480955\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 2 Train Loss: 266.254165294003 Valid Loss: 262.38534770213175 Test Loss: 246.37612120156214\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 3 Train Loss: 263.40250951397064 Valid Loss: 260.1576067409629 Test Loss: 257.0933387883477\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 4 Train Loss: 260.93643752523406 Valid Loss: 259.0970037226917 Test Loss: 257.9176423762695\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 5 Train Loss: 260.9101083487463 Valid Loss: 257.24624889484596 Test Loss: 255.6850878075846\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 6 Train Loss: 257.5497512998351 Valid Loss: 255.71670206549268 Test Loss: 253.90745265897309\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Epoch: 7 Train Loss: 261.8120855437656 Valid Loss: 256.45154058607545 Test Loss: 259.3582964035251\n",
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdcrnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_grad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mminimum_lr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(dcrnn, g, train_loader, normalizer, loss_fn, device, batch_size)\n\u001B[0;32m      4\u001B[0m     valid_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(dcrnn, g, valid_loader, normalizer, loss_fn, device, batch_size)\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\graph_traffic\\graph_traffic\\train.py:59\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, graph, dataloader, optimizer, scheduler, normalizer, loss_fn, device, batch_size, max_grad_norm, minimum_lr)\u001B[0m\n\u001B[0;32m     57\u001B[0m y, y_pred \u001B[38;5;241m=\u001B[39m predict(x, y, batch_size, graph, model, device, normalizer)\n\u001B[0;32m     58\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y)\n\u001B[1;32m---> 59\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_grad_norm)\n\u001B[0;32m     61\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\madrid-traffic\\env\\lib\\site-packages\\torch\\autograd\\function.py:243\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[1;32m--> 243\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m    244\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[0;32m    246\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    247\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    train(dcrnn, g, train_loader, optimizer, scheduler, normalizer, loss_fn, device, batch_size, max_grad_norm, minimum_lr)\n",
    "    train_loss = eval(dcrnn, g, train_loader, normalizer, loss_fn, device, batch_size)\n",
    "    valid_loss = eval(dcrnn, g, valid_loader, normalizer, loss_fn, device, batch_size)\n",
    "    test_loss = eval(dcrnn, g, test_loader, normalizer, loss_fn, device, batch_size)\n",
    "    print(f\"Epoch: {e} Train Loss: {train_loss} Valid Loss: {valid_loss} Test Loss: {test_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Save model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(dcrnn.state_dict(), f\"{project_path}/models/dcrnn.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}